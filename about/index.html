<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"
    />
    <title>About - Captain's Journal</title>
    <link rel="stylesheet" href="../styles.css" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;600&display=swap"
      rel="stylesheet"
    />
    <link
      href="https://fonts.googleapis.com/css2?family=Pirata+One&display=swap"
      rel="stylesheet"
    />
    <link rel="icon" type="image/x-icon" href="/images/favicon-32x32.png" />
  </head>
  <body>
    <!-- Navigation -->
    <nav class="main-nav">
      <div class="nav-container">
        <div class="nav-logo">
          <a href="/">
            <img src="/images/pirate-ship.svg" alt="Captain's Journal" />
            <span>Captain's Journal</span>
          </a>
        </div>
        <div class="nav-links">
          <a href="/" class="nav-link">Home</a>
          <a href="/about" class="nav-link active">About</a>
        </div>
      </div>
    </nav>

    <!-- Hero Banner -->
    <header class="hero-banner about-banner">
      <div class="hero-content">
        <h1>About The Journal</h1>
      </div>
      <div class="banner-footer">
        <div class="tagline">The Story Behind the Stories</div>
      </div>
    </header>

    <!-- Main Content -->
    <div class="content-wrapper">
      <div class="about-container">
        <!-- Project Story Section -->
        <section class="about-section">
          <h2>The Project's Origins</h2>
          <div class="about-content">
            <p>
              Working professionally in the IT space, i'm
              always trying to keep up with the latest technologies, and the
              latest trends. It all started when i saw an interview with the CEO
              of Microsoft, Satya Nadella, talking about the future of AI and
              how it will change the work space as we know it. He was
              specifically talking about AI Agents and how they eventually will
              replace all SAS applications.
              <br />
              (For those interested heres the interview:
              <a
                href="https://www.youtube.com/watch?v=GuqAUv4UKXo"
                target="_blank"
                rel="noopener noreferrer"
                >Satya Nadella on AI Agents</a
              >) <br />
            </p>

            <p>
              I thought to myself "Wow that sounds like a bold statement" but i
              hadn't really looked much into AI agents myself.
              <br />
              <br />
              So i decided to look into it and see what all the fuss was about.
              I started to read about the latest tools and techniques, and how
              these "AI Agents" work. I purchased a subscription to openAI and
              started to play around with the API. I quickly discovered that
              with just a few lines of python code i could create an AI agent
              that could do simple tasks. I experimented with it for a few
              hours, giving it a few simple tasks to complete, but lacked
              inspiration to really leverage the full potential of the API.
            </p>
            <p>
              That was until i gave it the simple prompt -- "Your are a pirate,
              write a short story of your adventures"
            </p>
            <p>
              I was quite impressed with the results, from such a simple prompt!
              This is when i really started experimenting!
              <br />
              <br />
              Soon, before i knew it, i had created character profiles, back
              stories, locations, world building, pirate lore, and a whole lot
              more! I sat for hours, reading the stories my agent was
              telling me, and i was hooked!
              <br />
              <br />
              Thats when i had the thought, "I wonder if other people would
              enjoy these wacky stories as much as i have?" I had been looking
              for a creative project i could really sink my teeth into, and this
              seemed like a fun idea!
              <br />
              <br />
              and thus, the captains-journal was born.
            </p>

            <h2>Okay, heres what i'll do!</h2>
              I'll whip up a quick HTML page, host it in an Azure static website, write some simple code, host the text episodes in a Azure blobs storage and voila!
              <br>
              <br>
              Right ?
              <br>
              Right............... ?
              <br>
              <br>
              <br>
              mwahahha, Jake your poor little innocent flower, you have no idea what your about to get yourself into!
              <br>
              <br>
              <h2>Humble Beginnings</h2>
              Okay, i now have an AI agent that is convinced it's a captain of a pirate ship.
              <br> 
              It can now generate short stories in a text format, and output the .txt files locally on my machine.
              <br>
              This is cool, but i want to make a web series with daily episodes. Currently the agent can generates a short pirate story, save it as a .txt file, but next time the scrpt runs, it generates a new story and overwirtes the current .txt file.
              <br>    
              <br>
              Okay, lets make some changes to the code!
              <br>
              <br>
              Firstly, i need to update the agent itself.
              <br>
              Currently all the agent knows is, it is the captain of at pirate ship, and should tell short stories.
              I want to update the agent to be able to generate a new episode every day, the new episode should be a follow-up to the previous episode.
              If the last eipsode is captains_log_1.txt the next episode should be captains_log_2.txt and so on.
            <br>
              I started by updating the prompt for the agent.
            <br>
            <br>
              <img src="/images/prompt-update.png" alt="Prompt Update">
              <br>
              <br>


              I then made a few changes to the python script
              <br>
              The script now scans the local directy where the "captaions_log_X.txt files are created. If the script finds captains_log_1.txt it will then produce captains_log_2.txt, and so on.
              If no .txt files are found, the script will create a new episode.
              <br>
              <br>
              Okay, this is a good start, the agent is now able to generate a new episode every run, also name the file correctly, but the episodes are not consistant.
              <br>
               <br>
              If i want to make a web series with daily episodes. For this to work, its extremely important that episdoes are consistant. Each episode should be a continuation of the previous one.
              <br>
              It won't work if story arch keeps changing, characters are suddenly in different locations, doing different things etc...
              <br>
              <br>
              hmmmm what if i could generate a new episode every day, each episode following the same consistent story arch?
              <br>
              <br>
              <h3>Enter The Captains assistant!</h3>
              Okay, so i needed a way for the AI to keep track of the story, character arcs, locations visited, and especially what happened in the previous episode.
                <br>
                I managed to achieve this by adding an assistant to the prompt. The Assistant is responsible for reading all previous logs created in the story so far.
                <br>
                <br>
                So how does this work?
                <br>
                <br>
                Just like like the main agent, the assistant first checks the local file directory for any existing .txt files.
                Any existing .txt files found are passed to the assistant, the assistant then reads all exsisting .txt files and passes the content to the main agent.
                The main agent then receives a new prompt, with the assistant's memory as the context.
                <br>
                <br>
                In this case, if text files are found the agent will receive the prompt -  "Please write the next log entry, continuing the story from all previous logs while maintaining consistency in the story and character development" 
                <br>
                <br>
                If no text files were found, the agent will receivve the prompt - "You can write a complete log entry without worrying about length restrictions. Focus on maintaining story consistency and providing a complete narrative."
                <br>
                <br>
                I'm now able to generate consistant episodes, with a continous story arch.
                <br>
                Pretty cool right?
                <br>
                <br>
                but, now it's time to take it to the next level.
                <br>
                <br>
                <br>
                
              <br>  
              <img src="/images/captains_asistant.png" alt="Assistant">
              <h2>The Voice of the Captain</h2>
              This is where things start to get really interesting. (and also a little wacky üòÇ)
              <br>
              <br>
              I'm now able to generate a new episode every day, with a continous story arch, characters, locations, and a continous story.
              but it's just text.....
              <br>
              what if i could generate a audio version of each episode?
              <br>
              <br>
              Okay, time to hit the "Google-kung-fu" lets see what options i have!
              <br>
              <br>
              Oh snap! OpenAI has a text to speech API!
              <a href="https://platform.openai.com/docs/guides/text-to-speech" target="_blank" rel="noopener noreferrer">OpenAI Text to Speech API</a>
              <br>
              <br>
              Okay lets see what we can build!
              <br><br>
              After reading through the documentation it was actually pretty quick and easy to get a prototype up and running.
              <br>
              I went with the voice "Ash" as i felt this was the voice that sounded best for a narrator. However, i wansn't 100% happy with the voice, so i deciced to experiment a bit, and this is where things get a bit wacky üòÇ
              <br><br>
              First i tried cloneing my own voice, and creating an AI version of myself to read the text.
              This actually worked better than i expected, which made it all the more creepy! One thing is listening to your own voice, but listening to yourself say things you've neveer said, or read words completely different was just to weird for me.
              <br><br>
              For those interested, here is a little preview of this freak!

              <!-- Add the first audio player -->
              <div class="audio-preview">
                  <h3>Jake the Narrator</h3>
                  <audio controls>
                      <source src="/files/Jake_the_narrator.mp3" type="audio/mpeg">
                      Your browser does not support the audio element.
                  </audio>
              </div>

              <br>
              <br>
              Next up i toke the pirate approach!
              <br>
              I read a bunch of text in a pirate voice, and tried to create an ai version of the voice,
              Needless to say, this evening i got alot of strange looks from my better half, as i sat yelling pirate slurs into my microphone.
              <br><br>
              This it what It ended up producingüòÇüòÇ

              <!-- Add the second audio player -->
              <div class="audio-preview">
                  <h3>Jake the Pirate</h3>
                  <audio controls>
                      <source src="/files/Jake_the_pirate.mp3" type="audio/mpeg">
                      Your browser does not support the audio element.
                  </audio>
                  <br><br>
                  As i sit writing this blog, i'm still now entirely sure what voice i'm going to go with for the finished produt. I still have alot of experimentation to do with different API's and prodcuts, so i guess you'll have to wait and see!
              </div>

              <br>
              <br>
              <h2>Token Drama</h2>
              4096, a number that probablly doesn't mean anything to most people, but to me it was a number i needed to overcome!
              <br><br>
              So what is this mysterious number 4096 you ask? - 4096 is the "token limit" for the OpenAI API.
              <br>
              What this means is, the maximum output for each openAI API call can not exceed 4096 tokens.
              <br><br>
              So what are tokens and how are they calculated?
              <br><br>
              Tokens are calculated by the number of words in the text. Tokens are pieces of words or characters that the AI processes to understand and generate text.
              They are not the same as words but represent fragments of words, whole words, or even punctuation.
              <br>
              <br>
              In English, 1 token ‚âà 4 characters (on average).
              <br>
              A single word can be multiple tokens, especially if it's complex.
              <br>
              Short words like "a" or "is" count as 1 token.
              <br>
              Longer words like "artificially" may be split into multiple tokens.
              <br><br>
              For example:
              <br><br>
              <table></table>
              <br>
              "Hello"	= 1 token
              <br>
              "Captains"	= 1 token
              <br>
              "Extraordinary"	= 2 tokens
               <br>
              "Pirate's adventure!" =	4 tokens
               <br>
              "The treasure is near."	= 5 tokens
              <br><br>
              So if 1 token is on average 4 characterS in English, and the average English word is 5 characters long (Including spaces), we can estimate that the output legnth of 4096 tokens, should be around 3200 words.
              3200 words should be around 25 minutes of audio at normal narration pace of 130 words per minute (According to ChatGPT)
              <br><br>
              Wait a second! Why are my audio files only around 3 and a half minutes long ?!?!
              <br><br>
              Is my maths completely off? or is something else going on her?
              <br><br>
              Errrmmm..... i'll be right back, i need to figure out whats going on! (Yes i'm finding this out now as i write this blog)
              <br><br>
              Oh hi there! Im back!
              <br>
              Sure, for you, it's like i never left, but i've actually been gone for hours, yup, thats right, hours! Another rabit hole into the depth of AI.
              I learnt some stuff when i was gone.... wanna hear it ?
              <br><br>
              Okay heres the deal.....
              Turns out there is a direct correlation between the number of tokens in the input and the output.
              <br><br>
              How Input Size Affects Output
              <br><br>
              GPT-4 Turbo has a max context length of 128k tokens (input + output combined).
              <br>
              Output tokens are capped at 4096 per request (this is a separate limitation).
              <br>
              The more input tokens you use, the fewer output tokens are available.
              <br><br>
              <table class="token-limits">
                <thead>
                    <tr>
                        <th>Input Size (Tokens)</th>
                        <th>Max Possible Output (Tokens)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>1,000</td>
                        <td>4,096 ‚úÖ <span class="note">(Full response possible)</span></td>
                    </tr>
                    <tr>
                        <td>3,000</td>
                        <td>4,096 ‚úÖ <span class="note">(Full response possible)</span></td>
                    </tr>
                    <tr>
                        <td>10,000</td>
                        <td>4,096 ‚úÖ <span class="note">(Still full output possible, as long as the total stays under 128k)</span></td>
                    </tr>
                    <tr>
                        <td>120,000</td>
                        <td>8,000 ‚ö†Ô∏è <span class="note">(Limited output tokens remaining)</span></td>
                    </tr>
                    <tr>
                        <td>125,000</td>
                        <td>3,000 ‚ö†Ô∏è <span class="note">(Very limited output tokens remaining)</span></td>
                    </tr>
                    <tr>
                        <td>127,000</td>
                        <td>1,000 ‚ö†Ô∏è <span class="note">(Critically low output tokens remaining)</span></td>
                    </tr>
                    <tr>
                        <td>128,000</td>
                        <td>0 ‚ùå <span class="note">(No room for output! Model won't generate anything)</span></td>
                    </tr>
                </tbody>
            </table>
              <br>
              Oh dear, maybe i'm using to many tokens on my input, and thats the reason my output is so short.
              <br>
              Okay back to coding, there must be a way to to an overview of the token usage.
              <br>
              Alright cool, manged to setup some code to monitor token usage, lets run some tests and see how we're looking!
              <br>
              <br>
              <img src="/images/token-usage.png" alt="Token Usage">
              <br>
              <br>
              Okay according to my logs, i'm not even close to hitting 120k tokens (Which is the point where output limits would start to accour).
              <br><br>
              So what exactly is happening here?
              <br>
              Honestly i don't know, but i do know that i'm not using 120k tokens on my input. From what i can tell, the agent maybe just feels that the entry it has given is adequate, and doesn't need to add any more detail.
              <br><br>
              Prehaps i'm overcomplicating things. Maybe i should tell the agent to make each story be 4095 tokens in length.
              <br><br>
            
              <img src="/images/Giveme4096tokens.png" alt="Give 4095 tokens">
              <br><br>
              At the same time i also updated the code to actaully display the toke size of the output. (Ouput tokens)
              <br><br>
              Lets see how this works!
              <br>
              <br>
              Test 1:
              <br>
              <img src="/images/outputtokens1.png" alt="Token Usage">
              <br>
              Test 2:
              <br>
              <img src="/images/outputtokens2.png" alt="Token Usage">
              <br><br>
              Wow, not even close to 4096! It completely ignored my input, even though i specifically stated i want to use all 4096 tokens.
              Just to elaborate, it's not that i want it to output exactly 4096 tokens each time, that would be way to long for a daily episodeic adventure, but i definitely want more than 933 tokens..
              <br><br>
              Alright back to the drawing board, time to read up on the OpenAI documentation *AGAIN* and see if i can find anything else that can help me out.
              <br>
              Okay, turns out there is something called a"Stop_reason" parameter in the API.
              <br>
              Seems there can be mutiple reasons why the model stops generating.
              <br>
              <br>
             *stop* = The Model chose to stop early
              <br>
              *length* = The Model hit the max token limit
              <br>
              *content_filter* = OpenAI blocked part of the output
              <br> 
              *null* = The Model stopped but no reason given
              <br>
              <br>
              Again, time to add more dedugging and logging to the code, lets see if we can find the reason why the model is stopping.
              <br>
              <br>                 
              <img src="/images/finishreason.png" alt="Stop Reason">
              <br>
              <br>
              WELL SON OF A BITCH! IT JUST STOPPED, "BECAUSE IT FELT LIKE IT", EVEN THOUGH DADDY SPECIFICALLY SAID "GIVE ME 4096 TOKENS"
              <br>
              <br>
              Okay lets try giving it the stop=none parameter. This means the model shouldn't stop no matter what.
              <br><br>
              <img src="/images/stopnone.png" alt="Stop None">
              <br><br>
              We'll also give it a little more convincing via a new prompt.
              <br><br>
             <img src="/images/dontstopearly.png" alt="Don't stop early">
             <br><br>
             YOU OW ME 4096 TOKENS, AND DADDY WANTS HIS TOKENS! - Just like Brad pit wants his scalps...
             <br>
             (This is Inglorious bastards reference, if you haven't seen the film, stop reading this blog and go watch it NOW!)
             <br>
             <br>
             <img src="/images/disobeyed.png" alt="Disobeyed">
             <br>
             Disobeyed by my creation once again... 1161 tokens....
             <br>
             <br>
             "Cocks Shotgun"
             <br>
             <br>
             Iv'e played with this for a few hours now, and i'm not able to get the model to ouput anywhere near to 4096 tokens, no matter how hard i try. I have however managed to get longer outputs in general.
             I'm now averaging around 1200 tokens per episode, which is around 5 minutes of audio, so that's a slight improvement.
             <br>
             <br>
             One thing i have noticed however, is that the model seems to generate longer and longer outputs as the series goes on.
             <br>
             I'm presuming this is probably because i'm constantly feeding the model with the previous episodes, giving it more and more context.
             <br>
             So presumably, the series will get more and more detailed as it goes on.
             <br>
             <br>
             I may re-vist this later, but for now i'm going to focus on the next set of problems.
             <br>
             <br>
             <h2>Temperature</h2>
             <p>
               Before I completely move on from this token drama, I'd like to touch quickly on the "Temperature" parameter, as this can essentially also effect the output length.
             </p>
             <p>
               Temperature is a key setting in AI models (like GPT-4 Turbo) that controls how random or predictable the AI's responses are. It affects the creativity, coherence, and unpredictability of the output.
             </p>
             <p>
               The temperature parameter is a float between 0 and 1.
             </p>

             <div class="temperature-section">
               <h3>üî• How Temperature Works</h3>
               
               <div class="temperature-level">
                 <h4>Low Temperature (0.1 - 0.4) ‚Üí More Predictable, Logical Responses</h4>
                 <ul>
                   <li>The AI sticks to safer, more deterministic answers</li>
                   <li>Good for technical, fact-based, or structured tasks</li>
                   <li>Example: AI follows strict patterns and rarely improvises</li>
                 </ul>
               </div>

               <div class="temperature-level">
                 <h4>Medium Temperature (0.5 - 0.7) ‚Üí Balanced Responses</h4>
                 <ul>
                   <li>A mix of coherence and creativity</li>
                   <li>Ideal for storytelling, brainstorming, and engaging conversations</li>
                   <li>Example: AI follows logical structures but adds some variation and personality</li>
                 </ul>
               </div>

               <div class="temperature-level">
                 <h4>High Temperature (0.8 - 1.2) ‚Üí Wild, Creative, Unpredictable</h4>
                 <ul>
                   <li>The AI takes risks and improvises more</li>
                   <li>Great for jokes, poetry, and wacky storytelling</li>
                   <li>Example: AI generates surreal, chaotic, or unexpected responses</li>
                 </ul>
               </div>

               <div class="pirate-example">
                 <h3>üé≠ How Temperature Affects a Pirate AI</h3>
                 <p>Let's say we ask the AI: "What does a pirate say when he finds buried treasure?"</p>

                 <div class="temperature-example">
                   <h4>üö® Low Temperature (0.2)</h4>
                   <p class="response">"Arrr, we found the treasure! Let's take it back to the ship."</p>
                   <p class="note">(Boring, expected, factual.)</p>
                 </div>

                 <div class="temperature-example">
                   <h4>‚öñÔ∏è Medium Temperature (0.6)</h4>
                   <p class="response">"Shiver me timbers! This be a chest of gold fit for a king! Now who be havin' a key?"</p>
                   <p class="note">(More personality, but still structured.)</p>
                 </div>

                 <div class="temperature-example">
                   <h4>üî• High Temperature (1.0+)</h4>
                   <p class="response">"Blimey! It be a chest full o' socks! NOOO! The map was upside down again!"</p>
                   <p class="note">(Completely chaotic and hilarious!)</p>
                   <br>
                   I'm building a chaotic comedic pirate adventure, i think we all know which setting i'll be going with!
                 </div>
               </div>
             </div>
             <h2>A quick recap</h2>
             Okay, lets quickly review where we're at!
             <br><br>
             We now have an AI agent thats convinced it's the most feared and legendary pirate to ever sail the seas. ‚úÖ 
             <br><br>
             It can produce a believable and engaging story, with a mix of personality and structure. ‚úÖ
             <br><br>
             It can now consistantly follow the story structure and keep story lines and characters consistent. ‚úÖ
             <br><br>
             We have a way to monitor the overall token usage. ‚úÖ
             <br><br>
             We have a way to control the temperature of the output. ‚úÖ
             <br>
             <br>
             Outputs are generally larger than 1200 tokens, which is a good start! ‚úÖ
             <br><br>
             Model stil defies daddys orders at giving an output of anything close to 4096 tokens! ‚ùå
             <br><br>
             <h2>Txt Chunks n' Audio Chunks -  The Departening!</h2>
             Alright, another day another problem!
             <br><br>
             With the model now consistantly outputting a good length of text, a new issues arises.
             <br>
             <br>
             Turns out the OpenAI TTS (text to speach) API has a max input limit of 4096 Characters per request, which is aprroximately 5 minutes of audio.
             <br>
             Some of the journal entries are over 4096 characters, but some are not......
             <br>
             The ones that are over 4096 are getting automatically truncated when being passed to the TTS API.
             <br>
             This means, if a journal entry is over 4096 (Lets say for example we have a journal entry of 7000 characters)
             The TTS API will only pass the first 4096 characters to the API, generate the audio for that chunk, then just stop.
             <br>
             Giving you only 4096 characters of audio, and leaving the rest of the journal entry untold.
             <br><br>
             This is an issue...
             <br><br>
             Luckily this is afairly simple problem to solve.
             <br>
             We know the limit for each TTS submission can max be 4096 characters, so we just need to split each journal entry into chunks of less than 4096 characters.
            <br>
            (Remembner we are now talking about characters, NOT tokens)
            <br><br>
            Alright, lets play it safe and split each journal entry into chucks of 4000 characters, this way we are sure we will not hit the limit.
            <br>
            This can easily be done by doing a some simple string manipulation in the code.
            <br>
            Some of the important things to consider however, is making sure the string split doesn't accour mid word, or mid sentence.
            <br>
            I found the best way to do this, was to make sure each split happened after a full stop.
            <br>
            <br>
            <img src="/images/split.png" alt="Split">
            <br>
            <br>
             Okay great, we are now able to hear the full log, without missing any data!
             <br>
             but rarely are things so simple...... 
            <br>
             Beacuase the text journal entries are being split into seperate files, and than fed to the TTS API in chunks of 4000 characters, this means the audio files outputted are also split into 4000 character chunks.
             <br><br>
             Now we find ourselves in the situation, were we can have up to 3 seperate audio files for one journal entry.
             <br><br>
             This is no good... The plan is to have a web series, were the audio files will be streamed directly from the website.
             <br>
             This should be seemless for the end user, and they should only have to deal with one audio file per episode.
             <br><br>
             <img src="/images/depart.png" alt="Departing">
             <br><br>
             Shit......
             <br>
             <h2>A small sidetrack for context</h2>
             Okay, so at this point i knew i wanted to "stitch" the audio files together, but i had no idea how i was going to achieve this.
             <br><br>
             I think this is a good point in the blog to tell you a little about myself.
             <br>
             I already mentioned i work in the IT industry, and have done for many years, but......
            <h4>i'm not not a front end developer, i'm not a back end developer, and i'm certainly not a software engineer!</h4>
             I work as a Cloud Architect, where my primary focus is designing and implementing cloud solutions inside Microsoft 365 and Azure.
             <br>
             (These skills will will come into play later in the blog, but for now, i just want to focus on the code.)
             <br><br>
             My standard day mainly consists of designing and maintaining IT infrastructure, with a large focus on cybersecurity and compliance.
             <br>
             This means i have a pretty good grasp of Powershell, and creating simple scripts to assist in my daily tasks, but Python, javascript, and HTLM is not an area i'm extremely well versed in.
             <br><br>
             I think this is important context to have for the upcoming sections of this blog.
             <br>
             <br>
             Okay back to the problem at hand! - Stitching the audio files together.
             <br>
             <br>
             Time to hit the "Google-fu" and see if i can find a way to achieve this!
             <br><br>
             During my time googleing for a solution, i came accross an intresting peice of software, that was about to change the game....
             <br>
             <br>
             <h2>Cursor has entered the chat!</h2>
             <br>
             <br>
             <h2>The website!</h2>
             <br> 
             <br>
             <br>
             <h2>Migrating to Azure...</h2>
             <br>
              <br>
              <h2>Azure Functions, and new ways to work</h2>
              <br>
              <h2>Securing the code</h2>
              <br>
              <h2>Have i bitten off more than i can chew?</h2>
              <br>
              <h2>I think the website is is ready</h2>
              <br>
              <h2>The doubt...</h2>

              <br>
              <br>

              
          </div>
        </section>

        <!-- Technical Stack Section -->
        <section class="about-section">

          </pre>
          <div class="tech-stack">
            <div class="tech-item">
              <img src="/images/azure-logo.svg" alt="Microsoft Azure" />
              <span>Microsoft Azure</span>
            </div>
            <div class="tech-item">
              <img src="/images/ai-logo.svg" alt="AI Technology" />
              <span>AI Integration</span>
            </div>
            <!-- Add more tech stack items as needed -->
          </div>
        </section>

        <!-- Creator Section -->
        <section class="about-section creator-section">
          <h2>About the Creator</h2>
          <div class="creator-profile">
            <img
              src="/images/profile-photo.jpg"
              alt="Jake Poulsom"
              class="creator-image"
            />
            <div class="creator-info">
              <h3>Jake Poulsom</h3>
              <p class="creator-title">Microsoft 365 & Azure Expert</p>
              <p class="creator-bio">
                As a Microsoft technology enthusiast, I've combined my technical
                expertise with creative storytelling to bring you these unique
                pirate tales. When not coding or writing, I'm exploring new ways
                to use AI in creative projects.
              </p>
              <div class="creator-links">
                <a
                  href="https://www.linkedin.com/in/jake-poulsom-3561271b5/"
                  target="_blank"
                  rel="noopener noreferrer"
                  class="social-link"
                >
                  LinkedIn
                </a>
                <a
                  href="mailto:thecaptain@captains-journal.com"
                  class="social-link"
                >
                  Contact
                </a>
              </div>
            </div>
          </div>
        </section>
      </div>
    </div>

    <!-- Footer -->
    <footer class="site-footer">
      <!-- ... existing footer content ... -->
    </footer>

    <!-- Newsletter Modal -->
    <div id="newsletterModal" class="modal">
      <!-- ... existing newsletter modal content ... -->
    </div>

    <!-- Scripts -->
    <script src="../client.js"></script>
  </body>
</html>
