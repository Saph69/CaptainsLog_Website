<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"
    />
    <title>About - Captain's Journal</title>
    <link rel="stylesheet" href="../styles.css" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;600&display=swap"
      rel="stylesheet"
    />
    <link
      href="https://fonts.googleapis.com/css2?family=Pirata+One&display=swap"
      rel="stylesheet"
    />
    <link rel="icon" type="image/x-icon" href="/images/favicon-32x32.png" />
  </head>
  <body>
    <!-- Navigation -->
    <nav class="main-nav">
      <div class="nav-container">
        <div class="nav-logo">
          <a href="/">
            <img src="/images/pirate-ship.svg" alt="Captain's Journal" />
            <span>Captain's Journal</span>
          </a>
        </div>
        <div class="nav-links">
          <a href="/" class="nav-link">Home</a>
          <a href="/about" class="nav-link active">About</a>
        </div>
      </div>
    </nav>

    <!-- Hero Banner -->
    <header class="hero-banner about-banner">
      <div class="hero-content">
        <h1>About The Journal</h1>
      </div>
      <div class="banner-footer">
        <div class="tagline">The Story Behind the Stories</div>
      </div>
    </header>

    <!-- Main Content -->
    <div class="content-wrapper">
      <div class="about-container">
        <!-- Project Story Section -->
        <section class="about-section">
          <h2>The Project's Origins</h2>
          <div class="about-content">
            <p>
              Working professionally in the IT space, i'm
              always trying to keep up with the latest technologies, and the
              latest trends. It all started when i saw an interview with the CEO
              of Microsoft, Satya Nadella, talking about the future of AI and
              how it will change the work space as we know it. He was
              specifically talking about AI Agents and how they eventually will
              replace all SAS applications.
              <br />
              (For those interested heres the interview:
              <a
                href="https://www.youtube.com/watch?v=GuqAUv4UKXo"
                target="_blank"
                rel="noopener noreferrer"
                >Satya Nadella on AI Agents</a
              >) <br />
            </p>

            <p>
              I thought to myself "Wow that sounds like a bold statement" but i
              hadn't really looked much into AI agents myself.
              <br />
              <br />
              So i decided to look into it and see what all the fuss was about.
              I started to read about the latest tools and techniques, and how
              these "AI Agents" work. I purchased a subscription to openAI and
              started to play around with the API. I quickly discovered that
              with just a few lines of python code i could create an AI agent
              that could do simple tasks. I experimented with it for a few
              hours, giving it a few simple tasks to complete, but lacked
              inspiration to really leverage the full potential of the API.
            </p>
            <p>
              That was until i gave it the simple prompt -- "Your are a pirate,
              write a short story of your adventures"
            </p>
            <p>
              I was quite impressed with the results, from such a simple prompt!
              This is when i really started experimenting!
              <br />
              <br />
              Soon, before i knew it, i had created character profiles, back
              stories, locations, world building, pirate lore, and a whole lot
              more! I sat for hours, reading the stories my agent was
              telling me, and i was hooked!
              <br />
              <br />
              Thats when i had the thought, "I wonder if other people would
              enjoy these wacky stories as much as i have?" I had been looking
              for a creative project i could really sink my teeth into, and this
              seemed like a fun idea!
              <br />
              <br />
              and thus, the captains-journal was born.
            </p>

            <h2>Okay, heres what i'll do!</h2>
              I'll whip up a quick HTML page, host it in an Azure static website, write some simple code, host the text episodes in a Azure blobs storage and voila!
              <br>
              <br>
              Right ?
              <br>
              Right............... ?
              <br>
              <br>
              <br>
              mwahahha, Jake your poor little innocent flower, you have no idea what your about to get yourself into!
              <br>
              <br>
              <h2>Humble Beginnings</h2>
              Okay, i now have an AI agent that is convinced it's a captain of a pirate ship.
              <br> 
              It can now generate short stories in a text format, and output the .txt files locally on my machine.
              <br>
              This is cool, but i want to make a web series with daily episodes. Currently the agent can generates a short pirate story, save it as a .txt file, but next time the scrpt runs, it generates a new story and overwirtes the current .txt file.
              <br>    
              <br>
              Okay, lets make some changes to the code!
              <br>
              <br>
              Firstly, i need to update the agent itself.
              <br>
              Currently all the agent knows is, it is the captain of at pirate ship, and should tell short stories.
              I want to update the agent to be able to generate a new episode every day, the new episode should be a follow-up to the previous episode.
              If the last eipsode is captains_log_1.txt the next episode should be captains_log_2.txt and so on.
            <br>
              I started by updating the prompt for the agent.
            <br>
            <br>
              <img src="/images/prompt-update.png" alt="Prompt Update">
              <br>
              <br>


              I then made a few changes to the python script
              <br>
              The script now scans the local directy where the "captaions_log_X.txt files are created. If the script finds captains_log_1.txt it will then produce captains_log_2.txt, and so on.
              If no .txt files are found, the script will create a new episode.
              <br>
              <br>
              Okay, this is a good start, the agent is now able to generate a new episode every run, also name the file correctly, but the episodes are not consistant.
              <br>
               <br>
              If i want to make a web series with daily episodes. For this to work, its extremely important that episdoes are consistant. Each episode should be a continuation of the previous one.
              <br>
              It won't work if story arch keeps changing, characters are suddenly in different locations, doing different things etc...
              <br>
              <br>
              hmmmm what if i could generate a new episode every day, each episode following the same consistent story arch?
              <br>
              <br>
              <h3>Enter The Captains assistant!</h3>
              Okay, so i needed a way for the AI to keep track of the story, character arcs, locations visited, and especially what happened in the previous episode.
                <br>
                I managed to achieve this by adding an assistant to the prompt. The Assistant is responsible for reading all previous logs created in the story so far.
                <br>
                <br>
                So how does this work?
                <br>
                <br>
                Just like like the main agent, the assistant first checks the local file directory for any existing .txt files.
                Any existing .txt files found are passed to the assistant, the assistant then reads all exsisting .txt files and passes the content to the main agent.
                The main agent then receives a new prompt, with the assistant's memory as the context.
                <br>
                <br>
                In this case, if text files are found the agent will receive the prompt -  "Please write the next log entry, continuing the story from all previous logs while maintaining consistency in the story and character development" 
                <br>
                <br>
                If no text files were found, the agent will receivve the prompt - "You can write a complete log entry without worrying about length restrictions. Focus on maintaining story consistency and providing a complete narrative."
                <br>
                <br>
                I'm now able to generate consistant episodes, with a continous story arch.
                <br>
                Pretty cool right?
                <br>
                <br>
                but, now it's time to take it to the next level.
                <br>
                <br>
                <br>
                
              <br>  
              <img src="/images/captains_asistant.png" alt="Assistant">
              <h2>The Voice of the Captain</h2>
              This is where things start to get really interesting. (and also a little wacky üòÇ)
              <br>
              <br>
              I'm now able to generate a new episode every day, with a continous story arch, characters, locations, and a continous story.
              but it's just text.....
              <br>
              what if i could generate a audio version of each episode?
              <br>
              <br>
              Okay, time to hit the "Google-kung-fu" lets see what options i have!
              <br>
              <br>
              Oh snap! OpenAI has a text to speech API!
              <a href="https://platform.openai.com/docs/guides/text-to-speech" target="_blank" rel="noopener noreferrer">OpenAI Text to Speech API</a>
              <br>
              <br>
              Okay lets see what we can build!
              <br><br>
              After reading through the documentation it was actually pretty quick and easy to get a prototype up and running.
              <br>
              I went with the voice "Ash" as i felt this was the voice that sounded best for a narrator. However, i wansn't 100% happy with the voice, so i deciced to experiment a bit, and this is where things get a bit wacky üòÇ
              <br><br>
              First i tried cloneing my own voice, and creating an AI version of myself to read the text.
              This actually worked better than i expected, which made it all the more creepy! One thing is listening to your own voice, but listening to yourself say things you've neveer said, or read words completely different was just to weird for me.
              <br><br>
              For those interested, here is a little preview of this freak!

              <!-- Add the first audio player -->
              <div class="audio-preview">
                  <h3>Jake the Narrator</h3>
                  <audio controls>
                      <source src="/files/Jake_the_narrator.mp3" type="audio/mpeg">
                      Your browser does not support the audio element.
                  </audio>
              </div>

              <br>
              <br>
              Next up i toke the pirate approach!
              <br>
              I read a bunch of text in a pirate voice, and tried to create an ai version of the voice,
              Needless to say, this evening i got alot of strange looks from my better half, as i sat yelling pirate slurs into my microphone.
              <br><br>
              This it what It ended up producingüòÇüòÇ

              <!-- Add the second audio player -->
              <div class="audio-preview">
                  <h3>Jake the Pirate</h3>
                  <audio controls>
                      <source src="/files/Jake_the_pirate.mp3" type="audio/mpeg">
                      Your browser does not support the audio element.
                  </audio>
                  <br><br>
                  As i sit writing this blog, i'm still now entirely sure what voice i'm going to go with for the finished produt. I still have alot of experimentation to do with different API's and prodcuts, so i guess you'll have to wait and see!
              </div>

              <br>
              <br>
              <h2>Token Drama</h2>
              4096, a number that probablly doesn't mean anything to most people, but to me it was a number i needed to overcome!
              <br><br>
              So what is this mysterious number 4096 you ask? - 4096 is the "token limit" for the OpenAI API.
              <br>
              What this means is, the maximum output for each openAI API call can not exceed 4096 tokens.
              <br><br>
              So what are tokens and how are they calculated?
              <br><br>
              Tokens are calculated by the number of words in the text. Tokens are pieces of words or characters that the AI processes to understand and generate text.
              They are not the same as words but represent fragments of words, whole words, or even punctuation.
              <br>
              <br>
              In English, 1 token ‚âà 4 characters (on average).
              <br>
              A single word can be multiple tokens, especially if it's complex.
              <br>
              Short words like "a" or "is" count as 1 token.
              <br>
              Longer words like "artificially" may be split into multiple tokens.
              <br><br>
              For example:
              <br><br>
              <table></table>
              <br>
              "Hello"	= 1 token
              <br>
              "Captains"	= 1 token
              <br>
              "Extraordinary"	= 2 tokens
               <br>
              "Pirate's adventure!" =	4 tokens
               <br>
              "The treasure is near."	= 5 tokens
              <br><br>
              So if 1 token is on average 4 characterS in English, and the average English word is 5 characters long (Including spaces), we can estimate that the output legnth of 4096 tokens, should be around 3200 words.
              3200 words should be around 25 minutes of audio at normal narration pace of 130 words per minute (According to ChatGPT)
              <br><br>
              Wait a second! Why are my audio files only around 3 and a half minutes long ?!?!
              <br><br>
              Is my maths completely off? or is something else going on her?
              <br><br>
              Errrmmm..... i'll be right back, i need to figure out whats going on! (Yes i'm finding this out now as i write this blog)
              <br><br>
              Oh hi there! Im back!
              <br>
              Sure, for you, it's like i never left, but i've actually been gone for hours, yup, thats right, hours! Another rabit hole into the depth of AI.
              I learnt some stuff when i was gone.... wanna hear it ?
              <br><br>
              Okay heres the deal.....
              Turns out there is a direct correlation between the number of tokens in the input and the output.
              <br><br>
              How Input Size Affects Output
              <br><br>
              GPT-4 Turbo has a max context length of 128k tokens (input + output combined).
              <br>
              Output tokens are capped at 4096 per request (this is a separate limitation).
              <br>
              The more input tokens you use, the fewer output tokens are available.
              <br><br>
              <table class="token-limits">
                <thead>
                    <tr>
                        <th>Input Size (Tokens)</th>
                        <th>Max Possible Output (Tokens)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>1,000</td>
                        <td>4,096 ‚úÖ <span class="note">(Full response possible)</span></td>
                    </tr>
                    <tr>
                        <td>3,000</td>
                        <td>4,096 ‚úÖ <span class="note">(Full response possible)</span></td>
                    </tr>
                    <tr>
                        <td>10,000</td>
                        <td>4,096 ‚úÖ <span class="note">(Still full output possible, as long as the total stays under 128k)</span></td>
                    </tr>
                    <tr>
                        <td>120,000</td>
                        <td>8,000 ‚ö†Ô∏è <span class="note">(Limited output tokens remaining)</span></td>
                    </tr>
                    <tr>
                        <td>125,000</td>
                        <td>3,000 ‚ö†Ô∏è <span class="note">(Very limited output tokens remaining)</span></td>
                    </tr>
                    <tr>
                        <td>127,000</td>
                        <td>1,000 ‚ö†Ô∏è <span class="note">(Critically low output tokens remaining)</span></td>
                    </tr>
                    <tr>
                        <td>128,000</td>
                        <td>0 ‚ùå <span class="note">(No room for output! Model won't generate anything)</span></td>
                    </tr>
                </tbody>
            </table>
              <br>
              Oh dear, maybe i'm using to many tokens on my input, and thats the reason my output is so short.
              <br>
              Okay back to coding, there must be a way to to an overview of the token usage.
              <br>
              Alright cool, manged to setup some code to monitor token usage, lets run some tests and see how we're looking!
              <br>
              <br>
              <img src="/images/token-usage.png" alt="Token Usage">
              <br>
              <br>
              Okay according to my logs, i'm not even close to hitting 120k tokens (Which is the point where output limits would start to accour).
              <br><br>
              So what exactly is happening here?
              <br>
              Honestly i don't know, but i do know that i'm not using 120k tokens on my input. From what i can tell, the agent maybe just feels that the entry it has given is adequate, and doesn't need to add any more detail.
              <br><br>
              Prehaps i'm overcomplicating things. Maybe i should tell the agent to make each story be 4095 tokens in length.
              <br><br>
            
              <img src="/images/Giveme4096tokens.png" alt="Give 4095 tokens">
              <br><br>
              At the same time i also updated the code to actaully display the toke size of the output. (Ouput tokens)
              <br><br>
              Lets see how this works!
              <br>
              <br>
              Test 1:
              <br>
              <img src="/images/outputtokens1.png" alt="Token Usage">
              <br>
              Test 2:
              <br>
              <img src="/images/outputtokens2.png" alt="Token Usage">
              <br><br>
              Wow, not even close to 4096! It completely ignored my input, even though i specifically stated i want to use all 4096 tokens.
              Just to elaborate, it's not that i want it to output exactly 4096 tokens each time, that would be way to long for a daily episodeic adventure, but i definitely want more than 933 tokens..
              <br><br>
              Alright back to the drawing board, time to read up on the OpenAI documentation *AGAIN* and see if i can find anything else that can help me out.
              <br>
              Okay, turns out there is something called a"Stop_reason" parameter in the API.
              <br>
              Seems there can be mutiple reasons why the model stops generating.
              <br>
              <br>
             *stop* = The Model chose to stop early
              <br>
              *length* = The Model hit the max token limit
              <br>
              *content_filter* = OpenAI blocked part of the output
              <br> 
              *null* = The Model stopped but no reason given
              <br>
              <br>
              Again, time to add more dedugging and logging to the code, lets see if we can find the reason why the model is stopping.
              <br>
              <br>                 
              <img src="/images/finishreason.png" alt="Stop Reason">
              <br>
              <br>
              WELL SON OF A BITCH! IT JUST STOPPED, "BECAUSE IT FELT LIKE IT", EVEN THOUGH DADDY SPECIFICALLY SAID "GIVE ME 4096 TOKENS"
              <br>
              <br>
              Okay lets try giving it the stop=none parameter. This means the model shouldn't stop no matter what.
              <br><br>
              <img src="/images/stopnone.png" alt="Stop None">
              <br><br>
              We'll also give it a little more convincing via a new prompt.
              <br><br>
             <img src="/images/dontstopearly.png" alt="Don't stop early">
             <br><br>
             YOU OW ME 4096 TOKENS, AND DADDY WANTS HIS TOKENS! - Just like Brad pit wants his scalps...
             <br>
             (This is Inglorious bastards reference, if you haven't seen the film, stop reading this blog and go watch it NOW!)
             <br>
             <br>
             <img src="/images/disobeyed.png" alt="Disobeyed">
             <br>
             Disobeyed by my creation once again... 1161 tokens....
             <br>
             <br>
             "Cocks Shotgun"
             <br>
             <br>
             Iv'e played with this for a few hours now, and i'm not able to get the model to ouput anywhere near to 4096 tokens, no matter how hard i try. I have however managed to get longer outputs in general.
             I'm now averaging around 1200 tokens per episode, which is around 5 minutes of audio, so that's a slight improvement.
             <br>
             <br>
             One thing i have noticed however, is that the model seems to generate longer and longer outputs as the series goes on.
             <br>
             I'm presuming this is probably because i'm constantly feeding the model with the previous episodes, giving it more and more context.
             <br>
             So presumably, the series will get more and more detailed as it goes on.
             <br>
             <br>
             I may re-vist this later, but for now i'm going to focus on the next set of problems.
             <br>
             <br>
             <br>
             <br>
             <h2>Audio Chunks</h2>
             <br>
             <h2>Migrating to Azure...</h2>
             <br>
              <br>
              <h2>Azure Functions, and new ways to work</h2>
              <br>
              <h2>Securing the code</h2>
              <br>
              <h2>Have i bitten off more than i can chew?</h2>
              <br>
              <h2>I think the website is is ready</h2>
              <br>
              <h2>The doubt...</h2>

              <br>
              <br>

              
          </div>
        </section>

        <!-- Technical Stack Section -->
        <section class="about-section">

          </pre>
          <div class="tech-stack">
            <div class="tech-item">
              <img src="/images/azure-logo.svg" alt="Microsoft Azure" />
              <span>Microsoft Azure</span>
            </div>
            <div class="tech-item">
              <img src="/images/ai-logo.svg" alt="AI Technology" />
              <span>AI Integration</span>
            </div>
            <!-- Add more tech stack items as needed -->
          </div>
        </section>

        <!-- Creator Section -->
        <section class="about-section creator-section">
          <h2>About the Creator</h2>
          <div class="creator-profile">
            <img
              src="/images/profile-photo.jpg"
              alt="Jake Poulsom"
              class="creator-image"
            />
            <div class="creator-info">
              <h3>Jake Poulsom</h3>
              <p class="creator-title">Microsoft 365 & Azure Expert</p>
              <p class="creator-bio">
                As a Microsoft technology enthusiast, I've combined my technical
                expertise with creative storytelling to bring you these unique
                pirate tales. When not coding or writing, I'm exploring new ways
                to use AI in creative projects.
              </p>
              <div class="creator-links">
                <a
                  href="https://www.linkedin.com/in/jake-poulsom-3561271b5/"
                  target="_blank"
                  rel="noopener noreferrer"
                  class="social-link"
                >
                  LinkedIn
                </a>
                <a
                  href="mailto:thecaptain@captains-journal.com"
                  class="social-link"
                >
                  Contact
                </a>
              </div>
            </div>
          </div>
        </section>
      </div>
    </div>

    <!-- Footer -->
    <footer class="site-footer">
      <!-- ... existing footer content ... -->
    </footer>

    <!-- Newsletter Modal -->
    <div id="newsletterModal" class="modal">
      <!-- ... existing newsletter modal content ... -->
    </div>

    <!-- Scripts -->
    <script src="../client.js"></script>
  </body>
</html>
